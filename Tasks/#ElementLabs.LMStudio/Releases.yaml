0.3.2:
  ReleaseNotes: |-
    What's new in 0.3.2
    - New: Ability to pin models to the top is back!
      - Right-click on a model in My Models and select "Pin to top" to pin it to the top of the list.
    - Chat migration dialog now appears in the chat side bar.
      - You can migrate your chats from pre-0.3.0 versions from there.
      - As of v0.3.1, system prompts now migrated as well.
      - Your old chats are NOT deleted.
    - Don't show a badge with a number on the downloads button if there are no downloads
    - Added a button to collapse the FAQ sidebar in the Discover tab
    - Reduced default context size from 8K to 4K tokens to alleviate Out of Memory issues
      - You can still configure any context size you want in the model load settings
    - Added a warning next to the Flash Attention in model load settings
      - Flash Attention is experimental and may not be suitable for all models
    - Updated the bundled llama.cpp engine to 3246fe84d78c8ccccd4291132809236ef477e9ea (Aug 27)
    Bug Fixes
    - Bug fix: My Models model size aggregation was incorrect when you had multi-part model files
    - Bug fix: (Linux) RAG would fail due to missing bundled embedding model (fixed)
    - Bug fix: Flash Attention - KV cache quantization is back to FP16 by default
      - In 0.3.0, the K and V were both set to Q8, which introduced large latencies in some cases
        - You might notice an increase in memory consumption when FA is ON compared with 0.3.1, but on par with 0.2.31
    - Bug fix: On some setups app would hang at start up (fix + mitigation)
    - Bug fix: Fixed an issue where the downloads panel was dragged over the app top bar
    - Bug fix: Fixed typos in built-in code snippets (server tab)
0.3.3:
  ReleaseNotes: |-
    What's new in 0.3.3
    - Configuration Presets
      - Save your system prompt, inference parameters as a preset.
      - Each chat can be associated with a preset.
    - Show live token count for user input using the selected model's tokenizer
    - Show live token count for system prompt using the selected model's tokenizer
    - New buttons at the top of the chat: Clear All Messages, Clone Chat
    - Stats underneath input box: show context fullness % (click on the label to toggle to n_tokens/n_context)
    - QoL: Chat auto-name, skip if time to first token high or tok/sec low
    - Perf: switching between long chats should be faster
    - First model download suggestion: suggest Gemma 2B for machines with less than 12GB of VRAM
    - Copy debug info to clipboard by right clicking the gear icon next to the model
    - Server tab: Code Snippets now have its own top level tab
    - Right click on links shows a context menu with options to open in browser or copy link
    New settings
    - Open App Settings using [‚åò,] or [Ctrl + ,] from anywhere
      - Settings icon is now in the bottom right corner of the app, no longer in the sidebar
      - [‚åò4] or [Ctrl + 4] now opens My Models
    - ThemeSelector ([‚åòK ‚åòT] or [Ctrl + K, Ctrl + T] from anywhere)
    - Choose App update channel: Stable or Beta
      - Choose between Stable and Beta in the App Settings
      - Beta versions might have the same version number as Stable versions, but the build number will be higher
    - Settings option to disable double-click to edit a chat message
    Bug Fixes
    - Bug fix: [Windows] fix app close/minimize/maximize buttons background overflow
    - Bug fix: Helpful error when the first message is longer than the total context size
    - Bug fix: Button to show / hide intermediate steps in RAG sometimes did not work (fixed)
    - Bug fix: [Developer] Checkbox for showing debug info did not work when unchecked (fixed)
    - Bug fix: Open downloads folder will open the custom downloads folder instead of the default one
    - Bug fix: Updated Jinja template parser to support models using more complex templates
    - Bug fix: Esc wouldn't close some dialogs
    - Bug fix: on first app usage, can't create a chat until a model is loaded
    - Bug fix: downloads popover was covering the settings dialog when both open
    - Bug fix: cmd + R / ctrl + R to regenerate sometimes wouldn't work
    Migration from 0.2.31 Presets
    - Pre-0.3 presets are automatically migrated to the new format upon save. The old files are NOT deleted.
    - Load parameters are not currently included in the new preset format.
      - Favor editing the model's default config in My Models.
    Big thank you to community localizers üôè
    - Spanish @xtianpaiva
    - Norwegian @Exlo84
    - German @marcelMaier
    - Turkish @progesor
    - Russian @shelomitsky, @mlatysh, @Adjacentai
    - Korean @williamjeong2
    - Polish @danieltechdev
    - Czech @ladislavsulc
    - Vietnamese @trinhvanminh
    - Portuguese (BR) @Sm1g00l
    - Portuguese (PT) @catarino
    - Chinese (zh-HK), (zh-TW), (zh-CN) @neotan
    - Chinese (zh-Hant) @kywarai
    - Ukrainian (uk) @hmelenok
    - Japanese (ja) @digitalsp
0.3.4:
  ReleaseNotes: |-
    What's new in 0.3.4
    New
    - [Mac] Support for MLX! Read the announcement here
    - Mission Control: Cmd+Shift+M to search for models, Cmd+Shift+R to manage LM Runtimes
    - Set Structured Output (JSON Schema) from the UI
    - [UI] Move Chat Appearance control to top bar
    - [UI] Tweaks to size of per-message action buttons
    Bug Fixes
    - Fix for Black Screen after prolonged use (reference: lmstudio-bug-tracker#98)
    - Fix for no port other than 1234 working for the local server (reference: lms#80)
    - Fix for embedding API not working from Obsidian (reference: tracker#142)
    - Fix for RAG sometimes failing during document processing
0.3.5:
  ReleaseNotes: |-
    0. 3.5 - Release Notes
    - Run LM Studio as a service (headless)
      - lms load, lms server start no longer requires launching the GUI
      - ability to run on machine startup
    - Server start / stop button will remember last setting
      - This is useful when LM Studio is running as a service
    - Improvement to Model Search
      - Hugging Face search now happens automatically without Cmd / Ctrl + Enter
    - Just-In-Time model loading for OpenAI endpoints
    - Button to toggle Mission Control full screen / modal modes
    - Update llama.cpp-based JSON response generation; now supports more complex JSON schemas
    - Tray menu options to minimize app to tray, copy server base URL
    - Checkbox to add lms to PATH during onboarding on Linux
    - [Mac][MLX Vision] Bump mlx-vlm version to 0.0.15, support Qwen2VL
    - [Mac][MLX Engine] Updated Transformers to 4.45.0
      - Fixes some issues with sideloading quantized MLX models (https://github.com/lmstudio-ai/mlx-engine/issues/10)
    - [UI] Move Chat Appearance control to top bar
    - [UI] Tweaks to size of per-message action buttons
    - Localization:
      - Improved German translation thanks to Goekdeniz-Guelmez
      - Indonesian translation thanks to dwirx
    Bug fixes
    - [Bug fix] fix RAG reinjecting document into context on follow up prompts
    - Fixed RAG not working (https://github.com/lmstudio-ai/mlx-engine/issues/4)
    - Fix outline flicker around Mission Control
